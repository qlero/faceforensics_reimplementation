@article{PRI,
year = {2008},
month = {06},
title = {Tampered photos},
volume = {88},
journal = {QPRI},
url = {https://archive.ph/20130704233149/http://www.pri.org/theworld/#selection-249.1-259.1}
}

@article{DBLP:journals/corr/abs-1803-09179,
  author    = {Andreas R{\"{o}}ssler and
               Davide Cozzolino and
               Luisa Verdoliva and
               Christian Riess and
               Justus Thies and
               Matthias Nie{\ss}ner},
  title     = {FaceForensics: {A} Large-scale Video Dataset for Forgery Detection
               in Human Faces},
  journal   = {CoRR},
  volume    = {abs/1803.09179},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.09179},
  eprinttype = {arXiv},
  eprint    = {1803.09179},
  timestamp = {Mon, 13 Aug 2018 16:46:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-09179.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{faceswap,
  author = {deepfakes},
  title = {FaceSwap},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/deepfakes/faceswap/network}},
  commit = {860ccb91acc9934a42264dbf283d86e142059e47}
}

@article{DBLP:journals/corr/KorshunovaSDT16,
  author    = {Iryna Korshunova and
               Wenzhe Shi and
               Joni Dambre and
               Lucas Theis},
  title     = {Fast Face-swap Using Convolutional Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1611.09577},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.09577},
  eprinttype = {arXiv},
  eprint    = {1611.09577},
  timestamp = {Mon, 13 Aug 2018 16:46:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KorshunovaSDT16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{8014966,

  author={Bondi, Luca and Lameri, Silvia and Güera, David and Bestagini, Paolo and Delp, Edward J. and Tubaro, Stefano},

  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 

  title={Tampering Detection and Localization Through Clustering of Camera-Based CNN Features}, 

  year={2017},

  volume={},

  number={},

  pages={1855-1864},

  doi={10.1109/CVPRW.2017.232}}

@article {dale2011video,
    title={Video Face Replacement},
    author={Dale, Kevin and Sunkavalli, Kalyan and Johnson, Micah K. and Vlasic, Daniel and Matusik, Wojciech and Pfister, Hanspeter},
    journal={ACM Trans. Graph.},
    issue_date={December 2011},
    volume={30},
    number={6},
    month={dec},
    year={2011},
    issn={0730-0301},
    pages={1--130},
    articleno={130},
    numpages={10},
    url={http://doi.acm.org/10.1145/2070781.2024164},
    doi={10.1145/2070781.2024164},
    acmid={2024164},
    publisher={ACM},
    address={New York, NY, USA},
    keywords={face replacement, facial animation, video compositing}
}

@article{10.1111/cgf.12552,
author = {Garrido, P. and Valgaerts, L. and Sarmadi, H. and Steiner, I. and Varanasi, K. and P\'{e}rez, P. and Theobalt, C.},
title = {VDub: Modifying Face Video of Actors for Plausible Visual Alignment to a Dubbed Audio Track},
year = {2015},
issue_date = {May 2015},
publisher = {The Eurographs Association &amp; John Wiley &amp; Sons, Ltd.},
address = {Chichester, GBR},
volume = {34},
number = {2},
issn = {0167-7055},
url = {https://doi.org/10.1111/cgf.12552},
doi = {10.1111/cgf.12552},
abstract = {In many countries, foreign movies and TV productions are dubbed, i.e., the original
voice of an actor is replaced with a translation that is spoken by a dubbing actor
in the country's own language. Dubbing is a complex process that requires specific
translations and accurately timed recitations such that the new audio at least coarsely
adheres to the mouth motion in the video. However, since the sequence of phonemes
and visemes in the original and the dubbing language are different, the video-to-audio
match is never perfect, which is a major source of visual discomfort. In this paper,
we propose a system to alter the mouth motion of an actor in a video, so that it matches
the new audio track. Our paper builds on high-quality monocular capture of 3D facial
performance, lighting and albedo of the dubbing and target actors, and uses audio
analysis in combination with a space-time retrieval method to synthesize a new photo-realistically
rendered and highly detailed 3D shape model of the mouth region to replace the target
performance. We demonstrate plausible visual quality of our results compared to footage
that has been professionally dubbed in the traditional way, both qualitatively and
through a user study.},
journal = {Comput. Graph. Forum},
month = may,
pages = {193–204},
numpages = {12}
}

@article{10.1145/2816795.2818056,
author = {Thies, Justus and Zollh\"{o}fer, Michael and Nie\ss{}ner, Matthias and Valgaerts, Levi and Stamminger, Marc and Theobalt, Christian},
title = {Real-Time Expression Transfer for Facial Reenactment},
year = {2015},
issue_date = {November 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/2816795.2818056},
doi = {10.1145/2816795.2818056},
abstract = {We present a method for the real-time transfer of facial expressions from an actor
in a source video to an actor in a target video, thus enabling the ad-hoc control
of the facial expressions of the target actor. The novelty of our approach lies in
the transfer and photorealistic re-rendering of facial deformations and detail into
the target video in a way that the newly-synthesized expressions are virtually indistinguishable
from a real video. To achieve this, we accurately capture the facial performances
of the source and target subjects in real-time using a commodity RGB-D sensor. For
each frame, we jointly fit a parametric model for identity, expression, and skin reflectance
to the input color and depth data, and also reconstruct the scene lighting. For expression
transfer, we compute the difference between the source and target expressions in parameter
space, and modify the target parameters to match the source expressions. A major challenge
is the convincing re-rendering of the synthesized target face into the corresponding
video stream. This requires a careful consideration of the lighting and shading design,
which both must correspond to the real-world environment. We demonstrate our method
in a live setup, where we modify a video conference feed such that the facial expressions
of a different person (e.g., translator) are matched in real-time.},
journal = {ACM Trans. Graph.},
month = oct,
articleno = {183},
numpages = {14},
keywords = {depth camera, faces, expression transfer, real-time}
}

@article{Thies2018Face2Face,
  title={Face2Face},
  author={Justus Thies and Michael Zollh{\"o}fer and Marc Stamminger and Christian Theobalt and Matthias Nie{\ss}ner},
  journal={Communications of the ACM},
  year={2018},
  volume={62},
  pages={96 - 104}
}

@article{Thies2016,
  title={Face2Face Presentation at CVPR 2016},
  author={Justus Thies},
  year={2016},
  url={https://on-demand.gputechconf.com/siggraph/2016/presentation/sig1641-justus-thies-matthias-niessner-face-to-face-real-time-capture-reenactment.pdf},
}

@article{10.1145/3182644,
author = {Thies, Justus and Zollh\"{o}fer, Michael and Stamminger, Marc and Theobalt, Christian and Nie\ss{}ner, Matthias},
title = {FaceVR: Real-Time Gaze-Aware Facial Reenactment in Virtual Reality},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {0730-0301},
url = {https://doi.org/10.1145/3182644},
doi = {10.1145/3182644},
abstract = {We propose FaceVR, a novel image-based method that enables video teleconferencing
in VR based on self-reenactment. State-of-the-art face tracking methods in the VR
context are focused on the animation of rigged 3D avatars (Li et al. 2015; Olszewski
et al. 2016). Although they achieve good tracking performance, the results look cartoonish
and not real. In contrast to these model-based approaches, FaceVR enables VR teleconferencing
using an image-based technique that results in nearly photo-realistic outputs. The
key component of FaceVR is a robust algorithm to perform real-time facial motion capture
of an actor who is wearing a head-mounted display (HMD), as well as a new data-driven
approach for eye tracking from monocular videos. Based on reenactment of a prerecorded
stereo video of the person without the HMD, FaceVR incorporates photo-realistic re-rendering
in real time, thus allowing artificial modifications of face and eye appearances.
For instance, we can alter facial expressions or change gaze directions in the prerecorded
target video. In a live setup, we apply these newly introduced algorithmic components.},
journal = {ACM Trans. Graph.},
month = jun,
articleno = {25},
numpages = {15},
keywords = {Face tracking, virtual reality, eye tracking}
}

@article{Thies_2018,
   title={Headon},
   volume={37},
   ISSN={1557-7368},
   url={http://dx.doi.org/10.1145/3197517.3201350},
   DOI={10.1145/3197517.3201350},
   number={4},
   journal={ACM Transactions on Graphics},
   publisher={Association for Computing Machinery (ACM)},
   author={Thies, Justus and Zollhöfer, Michael and Theobalt, Christian and Stamminger, Marc and Niessner, Matthias},
   year={2018},
   month={Aug},
   pages={1–13}
}

@misc{kim2018deep,
      title={Deep Video Portraits}, 
      author={Hyeongwoo Kim and Pablo Garrido and Ayush Tewari and Weipeng Xu and Justus Thies and Matthias Nießner and Patrick Pérez and Christian Richardt and Michael Zollhöfer and Christian Theobalt},
      year={2018},
      eprint={1805.11714},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{thies2019deferred,
      title={Deferred Neural Rendering: Image Synthesis using Neural Textures}, 
      author={Justus Thies and Michael Zollhöfer and Matthias Nießner},
      year={2019},
      eprint={1904.12356},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{10.1145/3072959.3073640,
author = {Suwajanakorn, Supasorn and Seitz, Steven M. and Kemelmacher-Shlizerman, Ira},
title = {Synthesizing Obama: Learning Lip Sync from Audio},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3072959.3073640},
doi = {10.1145/3072959.3073640},
abstract = {Given audio of President Barack Obama, we synthesize a high quality video of him speaking
with accurate lip sync, composited into a target video clip. Trained on many hours
of his weekly address footage, a recurrent neural network learns the mapping from
raw audio features to mouth shapes. Given the mouth shape at each time instant, we
synthesize high quality mouth texture, and composite it with proper 3D pose matching
to change what he appears to be saying in a target video to match the input audio
track. Our approach produces photorealistic results.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {95},
numpages = {13},
keywords = {big data, audio, videos, LSTM, lip sync, audiovisual speech, RNN, uncanny valley, face synthesis}
}

@article{elor2017bringingPortraits,
author = {Hadar Averbuch-Elor and  Daniel Cohen-Or and Johannes Kopf and Michael F. Cohen},
title = {Bringing Portraits to Life},
journal = {ACM Transactions on Graphics (Proceeding of SIGGRAPH Asia 2017)},
volume = {36},
number = {6},
pages = {196},
year = {2017},
publisher={ACM}
}

@misc{antipov2017face,
      title={Face Aging With Conditional Generative Adversarial Networks}, 
      author={Grigory Antipov and Moez Baccouche and Jean-Luc Dugelay},
      year={2017},
      eprint={1702.01983},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@INPROCEEDINGS{8237529,  author={Huang, Rui and Zhang, Shu and Li, Tianyu and He, Ran},  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)},   title={Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis},   year={2017},  volume={},  number={},  pages={2458-2467},  doi={10.1109/ICCV.2017.267}}

@article{article,
author = {Lu, Yongyi and Tai, Yu-Wing and Tang, Chi-Keung},
year = {2017},
month = {05},
pages = {},
title = {Conditional CycleGAN for Attribute Guided Face Image Generation}
}

@misc{upchurch2017deep,
      title={Deep Feature Interpolation for Image Content Changes}, 
      author={Paul Upchurch and Jacob Gardner and Geoff Pleiss and Robert Pless and Noah Snavely and Kavita Bala and Kilian Weinberger},
      year={2017},
      eprint={1611.05507},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{lample2018fader,
      title={Fader Networks: Manipulating Images by Sliding Attributes}, 
      author={Guillaume Lample and Neil Zeghidour and Nicolas Usunier and Antoine Bordes and Ludovic Denoyer and Marc'Aurelio Ranzato},
      year={2018},
      eprint={1706.00409},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{silva,
  title={An intuitive introduction to Generative Adversarial Networks (GANs)},
  author={Thalles Silva},
  year={2018},
  url={https://www.freecodecamp.org/news/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394/},
}

@INPROCEEDINGS{8014971,  author={Long, Chengjiang and Smith, Eric and Basharat, Arslan and Hoogs, Anthony},  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},   title={A C3D-Based Convolutional Neural Network for Frame Dropping Detection in a Single Video Shot},   year={2017},  volume={},  number={},  pages={1898-1906},  doi={10.1109/CVPRW.2017.237}}

@ARTICLE{7869361,  author={Ding, Xiangling and Yang, Gaobo and Li, Ran and Zhang, Lebing and Li, Yue and Sun, Xingming},  journal={IEEE Transactions on Circuits and Systems for Video Technology},   title={Identification of Motion-Compensated Frame Rate Up-Conversion Based on Residual Signals},   year={2018},  volume={28},  number={7},  pages={1497-1512},  doi={10.1109/TCSVT.2017.2676162}}

@ARTICLE{8288661,  author={D’Amiano, Luca and Cozzolino, Davide and Poggi, Giovanni and Verdoliva, Luisa},  journal={IEEE Transactions on Circuits and Systems for Video Technology},   title={A PatchMatch-Based Dense-Field Algorithm for Video Copy–Move Detection and Localization},   year={2019},  volume={29},  number={3},  pages={669-682},  doi={10.1109/TCSVT.2018.2804768}}

@INPROCEEDINGS{8296533,  author={Mullan, Patrick and Cozzolino, Davide and Verdoliva, Luisa and Riess, Christian},  booktitle={2017 IEEE International Conference on Image Processing (ICIP)},   title={Residual-based forensic comparison of video sequences},   year={2017},  volume={},  number={},  pages={1507-1511},  doi={10.1109/ICIP.2017.8296533}}

@INPROCEEDINGS{8267647,  author={Rahmouni, Nicolas and Nozick, Vincent and Yamagishi, Junichi and Echizen, Isao},  booktitle={2017 IEEE Workshop on Information Forensics and Security (WIFS)},   title={Distinguishing computer graphics from natural images using convolution neural networks},   year={2017},  volume={},  number={},  pages={1-6},  doi={10.1109/WIFS.2017.8267647}}

@INPROCEEDINGS{8014962,  author={Raghavendra, R. and Raja, Kiran B. and Venkatesh, Sushma and Busch, Christoph},  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},   title={Transferable Deep-CNN Features for Detecting Digital and Print-Scanned Morphed Face Images},   year={2017},  volume={},  number={},  pages={1822-1830},  doi={10.1109/CVPRW.2017.228}}

@ARTICLE{7349174,  author={Carvalho, Tiago and Faria, Fábio A. and Pedrini, Hélio and da S. Torres, Ricardo and Rocha, Anderson},  journal={IEEE Transactions on Information Forensics and Security},   title={Illuminant-Based Transformed Spaces for Image Forensics},   year={2016},  volume={11},  number={4},  pages={720-733},  doi={10.1109/TIFS.2015.2506548}}

@INPROCEEDINGS{8014963,  author={Zhou, Peng and Han, Xintong and Morariu, Vlad I. and Davis, Larry S.},  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},   title={Two-Stream Neural Networks for Tampered Face Detection},   year={2017},  volume={},  number={},  pages={1831-1839},  doi={10.1109/CVPRW.2017.229}}

@INPROCEEDINGS{8630787,  author={Li, Yuezun and Chang, Ming-Ching and Lyu, Siwei},  booktitle={2018 IEEE International Workshop on Information Forensics and Security (WIFS)},   title={In Ictu Oculi: Exposing AI Created Fake Videos by Detecting Eye Blinking},   year={2018},  volume={},  number={},  pages={1-7},  doi={10.1109/WIFS.2018.8630787}}

@INPROCEEDINGS{8639163,  author={Güera, David and Delp, Edward J.},  booktitle={2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)},   title={Deepfake Video Detection Using Recurrent Neural Networks},   year={2018},  volume={},  number={},  pages={1-6},  doi={10.1109/AVSS.2018.8639163}}

@ARTICLE{5734842,  author={Amerini, Irene and Ballan, Lamberto and Caldelli, Roberto and Del Bimbo, Alberto and Serra, Giuseppe},  journal={IEEE Transactions on Information Forensics and Security},   title={A SIFT-Based Forensic Method for Copy–Move Attack Detection and Transformation Recovery},   year={2011},  volume={6},  number={3},  pages={1099-1110},  doi={10.1109/TIFS.2011.2129512}}

@INPROCEEDINGS{7169839,  author={Zampoglou, Markos and Papadopoulos, Symeon and Kompatsiaris, Yiannis},  booktitle={2015 IEEE International Conference on Multimedia   Expo Workshops (ICMEW)},   title={Detecting image splicing in the wild (WEB)},   year={2015},  volume={},  number={},  pages={1-6},  doi={10.1109/ICMEW.2015.7169839}}

@INPROCEEDINGS{8638296,  author={Guan, Haiying and Kozak, Mark and Robertson, Eric and Lee, Yooyoung and Yates, Amy N. and Delgado, Andrew and Zhou, Daniel and Kheyrkhah, Timothee and Smith, Jeff and Fiscus, Jonathan},  booktitle={2019 IEEE Winter Applications of Computer Vision Workshops (WACVW)},   title={MFC Datasets: Large-Scale Benchmark Datasets for Media Forensic Challenge Evaluation},   year={2019},  volume={},  number={},  pages={63-72},  doi={10.1109/WACVW.2019.00018}}

@ARTICLE{7776959,  author={Korus, Paweł and Huang, Jiwu},  journal={IEEE Transactions on Information Forensics and Security},   title={Multi-Scale Analysis Strategies in PRNU-Based Tampering Localization},   year={2017},  volume={12},  number={4},  pages={809-824},  doi={10.1109/TIFS.2016.2636089}}

@misc{korshunov2018deepfakes,
      title={DeepFakes: a New Threat to Face Recognition? Assessment and Detection}, 
      author={Pavel Korshunov and Sebastien Marcel},
      year={2018},
      eprint={1812.08685},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{hui,
  title={How deep learning fakes videos (Deepfake) and how to detect it?},
  author={Jonathan Hui},
  year={2018},
  url={https://jonathan-hui.medium.com/how-deep-learning-fakes-videos-deepfakes-and-how-to-detect-it-c0b50fbf7cb9},
}

@misc{thies2020face2face,
      title={Face2Face: Real-time Face Capture and Reenactment of RGB Videos}, 
      author={Justus Thies and Michael Zollhöfer and Marc Stamminger and Christian Theobalt and Matthias Nießner},
      year={2020},
      eprint={2007.14808},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@ARTICLE{8335799,  author={Bayar, Belhassen and Stamm, Matthew C.},  journal={IEEE Transactions on Information Forensics and Security},   title={Constrained Convolutional Neural Networks: A New Approach Towards General Purpose Image Manipulation Detection},   year={2018},  volume={13},  number={11},  pages={2691-2706},  doi={10.1109/TIFS.2018.2825953}}

@inproceedings{10.1145/3082031.3083247,
author = {Cozzolino, Davide and Poggi, Giovanni and Verdoliva, Luisa},
title = {Recasting Residual-Based Local Descriptors as Convolutional Neural Networks: An Application to Image Forgery Detection},
year = {2017},
isbn = {9781450350617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3082031.3083247},
doi = {10.1145/3082031.3083247},
abstract = {Local descriptors based on the image noise residual have proven extremely effective
for a number of forensic applications, like forgery detection and localization. Nonetheless,
motivated by promising results in computer vision, the focus of the research community
is now shifting on deep learning. In this paper we show that a class of residual-based
descriptors can be actually regarded as a simple constrained convolutional neural
network (CNN). Then, by relaxing the constraints, and fine-tuning the net on a relatively
small training set, we obtain a significant performance improvement with respect to
the conventional detector.},
booktitle = {Proceedings of the 5th ACM Workshop on Information Hiding and Multimedia Security},
pages = {159–164},
numpages = {6},
keywords = {cnn, image forgery detection., bag-of-words, local descriptors},
location = {Philadelphia, Pennsylvania, USA},
}

@ARTICLE{6197267,  author={Fridrich, Jessica and Kodovsky, Jan},  journal={IEEE Transactions on Information Forensics and Security},   title={Rich Models for Steganalysis of Digital Images},   year={2012},  volume={7},  number={3},  pages={868-882},  doi={10.1109/TIFS.2012.2190402}}


@INPROCEEDINGS{8630761,  author={Afchar, Darius and Nozick, Vincent and Yamagishi, Junichi and Echizen, Isao},  booktitle={2018 IEEE International Workshop on Information Forensics and Security (WIFS)},   title={MesoNet: a Compact Facial Video Forgery Detection Network},   year={2018},  volume={},  number={},  pages={1-7},  doi={10.1109/WIFS.2018.8630761}}

@INPROCEEDINGS{8099678,  author={Chollet, François},  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Xception: Deep Learning with Depthwise Separable Convolutions},   year={2017},  volume={},  number={},  pages={1800-1807},  doi={10.1109/CVPR.2017.195}}

