<!DOCTYPE html>
<html>
  <head>
    <title>FaceForensics++: Learning to Detect Facial Images</title>
    <meta charset="utf-8">
    <style>
      /* @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic); */
      @import url(css/uca_metropolis.css);
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle, title-slide
count: false

## <b>FaceForensics++: Learning to Detect Manipulated Facial Images</b><br/><sub>A 2019 paper by A. R√∂ssler et al. (arXiv:1901.08971)</sub>
### Presented by Q. Le Roux (github@qlero)
<br/>
.less-line-height[
.grey[Universit√© C√¥te d'Azur]
]

---

# A Rising Concern

```
        facial expression manipulation
        + facial identity manipulation
          + ubiquitous neural networks
       -------------------------------
       significant concern for society
       (political, economic, societal)
```

<center><img src="images/putin.gif" width="45%" /></center>

.font50[
**Source**: `Kim, H. et al. Deep video portraits. ACM Trans. Graph. (TOG) 2018`
]

---

class: center, middle, title-slide

## Developing detection techniques has become cornerstone of the field of computer vision in recent years.

---

# The Paper's Motivation

<br/>
> To test the realism of state-of-the-art image manipulation

> To standardize the evaluation of detection methods

---

class: center, middle, title-slide

# üßÆ Techniques & State-of-the-Art

---

# Advances in Facial Digitization

<br/>
New techniques, better computers enable facial digitization in two different approaches:

- **Graphics-based**: Relies on expert-selected features and patterns
- **Learning-based**: Relies on neural networks

---

# An Overall Generic Process

<small>The two approaches usually follow the same process:

1. Identify the face region, isolate it
2. Modify the region and recombine

This process usually leaves artifacts, which detection systems might look for.</small>

<center><img src="images/process.png" width=100%/></center>

---

# Expression Manipulation

<small>State of the art:

- Graphics appr.: **NeuralTextures**, `Thies et al., 2019`
- Learning appr.: **Face2Face**, `Thies et al., 2016`

</small>

<center><img src="images/synthesizing_obama.gif" width = "70%"/></center>
<br/>
.font50[
**Sources**: `Thies, J. et al. Face2Face: Real-Time Face Capture & Reenactment. IEEE. 2016; Thies, J. et al. Deffered Neural Rendering. ACM Trans. Graph. (TOG) 2019`
]

---

# Identity Manipulation

<small>State of the art:

- Graphics appr.: **FaceSwap**, `Kowalski, 2016`
- Learning appr.: **DeepFakes**, `deepfakes, 2017` &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**FaceShifter**, `Li et al., 2020`

</small>

<center><img src="images/deepfake.gif" width = "40%"/></center>

.font50[
**Note**: FaceShifter was added in a 2020 revision of the paper.<br/>
**Sources**: `Kowalski, FaceSwap, GitHub, 2015; Deepfakes Gr., deepfakes, GitHub, 2016; Li et al. FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping. CVPR. 2020`
]
---

class: center, middle, title-slide

# üóÑÔ∏è The Paper's Approach

---

# The author's Objectives

<small>Beyond motivations, the authors set out to publish:

1. A large scale dataset to standardize the industry

2. A state-of-the-art forgery detector

3. An automated benchmark with a human baseline

<u>Overarching goal:</u>

**<center>To thoroughly test the state-of-the-art of facial forgeries</center>**

</small>

---

# The FaceForensics++ Dataset

<small><center>An order of magnitude larger than previous ones</center>
<br/>

| Origin | <div style="width:250px">Number of videos</div> | <div style="width:250px">Available quality</div> |
| -------------: | ----: | ----------: | 
| raw YouTube    | 1,000 | raw, HQ, LQ |
| Face2Face      | 1,000 | raw, HQ, LQ |
| DeepFakes      | 1,000 | raw, HQ, LQ |
| FaceSwap       | 1,000 | raw, HQ, LQ |
| NeuralTextures | 1,000 | raw, HQ, LQ |
| FaceShifter    | 1,000 | raw, HQ, LQ |
</small>

.font50[
**Note**: The original dataset (without FaceShifter) amounts to c. 1.8m images. HQ and LQ corresponds to levels of "constant rate quantization" compression, resp. 23 and 40.
]

---

# Problem Statement To Evaluate FF++

<small>**Human observer baseline**:
- 50:50 split between pristine and fake images
- 60 images per observer with an viewing time limit up to 6 seconds

**Problem statement for automated solution**: 
- 720/140/140 video split for training, val. and test.
- per-frame binary classification

</small>

---

# Xception, the then-best detector

---

class: center, middle, title-slide

# üß± Performance & Limitations

---

class: center, middle, title-slide

# üåê Future Directions

---

class: center, middle, title-slide

# ü§© It's been a pleasure

## Ask me questions!

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
